#!/usr/bin/env python
"""
rag_langchain_chat_history.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Retrieval-Augmented Generation demo built *entirely* with LangChain
components + OpenAI.

Key ingredients
---------------
â–ªï¸   TextLoader + PyPDFLoader            â†’ read .txt & .pdf  
â–ªï¸   RecursiveCharacterTextSplitter      â†’ chunk docs  
â–ªï¸   OpenAIEmbeddings                    â†’ embed chunks  
â–ªï¸   FAISS vector store (LangChain wrap) â†’ similarity search  
â–ªï¸   ConversationalRetrievalChain        â†’ chat w/ memory  
â–ªï¸   ConversationBufferMemory            â†’ keeps history

Install
-------
pip install \
    langchain-openai \
    langchain-community \
    faiss-cpu \
    pypdf \
    tqdm
"""

from pathlib import Path
import os, textwrap

# â”€â”€ LangChain pieces â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

from dotenv import load_dotenv
load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("openai_key", "")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DOCS_DIR        = Path(r"C:\Users\STEVE\Documents\GenAI\RAG_DATA")            # .txt & .pdf live here
EMBED_MODEL     = "text-embedding-3-small"
CHAT_MODEL      = "gpt-4o-mini"
CHUNK_SIZE      = 800
CHUNK_OVERLAP   = 150
PERSIST_PATH    = "faiss_store"          # folder created on first run

SYSTEM_PROMPT = (
    "You are a precise, concise tutor. "
    "Answer ONLY from the provided context. "
    "If the answer is missing, say â€œI don't know.â€"
)




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0.  API key  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "OPENAI_API_KEY" not in os.environ:
    raise SystemExit("ğŸ‘‰  export OPENAI_API_KEY=... and run again")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1.  Load docs  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_documents():
    loaders = []
    for path in DOCS_DIR.rglob("*.txt"):
        loaders.append(TextLoader(str(path)))
    for path in DOCS_DIR.rglob("*.pdf"):
        loaders.append(PyPDFLoader(str(path)))

    if not loaders:
        raise RuntimeError(f"No .txt or .pdf found inside {DOCS_DIR.absolute()}")

    docs = []
    for loader in loaders:
        docs.extend(loader.load())

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
    )
    return splitter.split_documents(docs)

docs = load_documents()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2.  Vector store  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embeddings = OpenAIEmbeddings(model=EMBED_MODEL)

if Path(PERSIST_PATH).exists():
    vectordb = FAISS.load_local(
        PERSIST_PATH, embeddings, allow_dangerous_deserialization=True
    )
else:
    vectordb = FAISS.from_documents(docs, embeddings)
    vectordb.save_local(PERSIST_PATH)

retriever = vectordb.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4},
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3.  Memory  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
memory = ConversationBufferMemory(
    memory_key='chat_history',
    return_messages=True,
    output_key='answer'
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4.  Chain  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
llm = ChatOpenAI(model=CHAT_MODEL, temperature=0.2)
# Define a custom prompt template
# âœ”ï¸ Use both "context" and "question" as expected by the chain
custom_prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(SYSTEM_PROMPT),
    HumanMessagePromptTemplate.from_template("Context:\n{context}\n\nQuestion: {question}")
])


# âœ”ï¸ Tell the chain to use this prompt and that "context" is the document variable
rag_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True,
    combine_docs_chain_kwargs={"prompt": custom_prompt}
)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5.  Chat loop  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    print("ğŸ”  Loading documents from", DOCS_DIR)
    print("ğŸ”  Building vector store (this may take a while) ..."
          )
    print("\nğŸ“š  Vector store ready. Ask away!  (Ctrl-C to quit)")

    try:
        while True:
            user_q = input("\nğŸ’¬  You: ")
            result = rag_chain({"question": user_q})

            answer   = result["answer"]
            sources  = result["source_documents"]

            # Show retrieved context for teaching transparency
            print("\nğŸ”  Retrieved context")
            print("â”€" * 60)
            for i, doc in enumerate(sources, 1):
                snippet = doc.page_content[:600].replace("\n", " ")
                print(textwrap.indent(textwrap.fill(snippet, 88), f"[Doc {i}] "))
            print("â”€" * 60)

            # Assistant reply
            print("ğŸ¤–  Assistant:\n")
            print(textwrap.fill(answer, width=88))

    except KeyboardInterrupt:
        print("\nBye!")
